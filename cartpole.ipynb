{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cartpole.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TvpX5UgId0JN","outputId":"e5953171-d1a3-44d6-8534-4b2abdfc6ef7"},"source":["'''\n","\n","Jiang Zhu\n","\n","Discription: A DQN Agent for CartPole openai gym environment\n","\n","Date: April 3 2021\n","\n","'''\n","\n","# Importing Libearies\n","import shutil\n","import os\n","from keras.models import Sequential, load_model\n","from keras.layers import Dense\n","import numpy as np\n","import gym\n","from collections import deque\n","import random\n","import time\n","\n","# Initializing environment and set hyperparameters\n","MODEL_FILE_PATH = '/content/drive/MyDrive/DQN_Model'\n","env = gym.make('CartPole-v1')\n","STATE_SHAPE = env.observation_space.shape[0]\n","NUM_ACTIONS = env.action_space.n\n","SAVE_MODEL_EVERY = 100\n","EPSILON_DECAY = 0.95\n","MIN_REPLAY_MEMORY_SIZE = 1000\n","MAX_REPLAY_MEMORY_SIZE = 5000\n","MIN_EPSILON = 0.01\n","PREDICTION_MODEL_UPDATE = 10\n","MINI_BATCH_SIZE = 32\n","NUM_EPISODES = 1001\n","RENDER_EVERY_EPISODE = 250\n","BEGIN_EPSILON = 1.0\n","UPDATE_PREDICTION_EVERY = 10\n","GAMMA = 0.95\n","\n","# The DQN agent class\n","class DQNAgent:\n","    def __init__(self):\n","        self.EPSILON = BEGIN_EPSILON\n","\n","        # final model we want to output\n","        self.target_model = self.create_model()\n","\n","        # the regularly updated model to create stationary objective function\n","        self.prediction_model = self.create_model()\n","        self.update_prediction_model()\n","\n","        # the replay memory that only keeps track of certain amount of previous memory\n","        # this deck will consist of tuples (state, action, reward, new_state, done)\n","        self.replay_memory = deque(maxlen = MAX_REPLAY_MEMORY_SIZE)\n","\n","        # counter on perdiction model update\n","        self.pred_model_update_counter = 0\n","\n","\n","\n","    def create_model(self):\n","        # Create a Deep NN model\n","        nn = Sequential()\n","        nn.add(Dense(4, input_dim = STATE_SHAPE, activation = 'relu'))\n","        nn.add(Dense(4,activation = 'relu'))\n","        nn.add(Dense(NUM_ACTIONS))\n","        nn.compile(optimizer='Adam', loss = 'mse', metrics=['accuracy'])\n","        return nn\n","\n","    def replay(self):\n","        # train the model only when we have enough replay memory\n","        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n","            return\n","        # Randomly sample from the replay memory\n","        minibatch = random.sample(self.replay_memory, MINI_BATCH_SIZE)\n","\n","        X = np.stack([memory_instance[0] for memory_instance in minibatch],axis = 1)[0]\n","        X_new_states = np.stack([memory_instance[3] for memory_instance in minibatch],axis = 1)[0]\n","        y = self.prediction_model.predict(X)\n","        y_new_states = self.prediction_model.predict(X_new_states)\n","        for i, (state, action, reward, new_state, done) in enumerate(minibatch):\n","          if done:\n","            y[i, action] = reward\n","          else:\n","            y[i, action] = reward + GAMMA * np.amax(y_new_states[i])\n","\n","        self.target_model.fit(X, y, batch_size=MINI_BATCH_SIZE,shuffle=False,epochs=1, verbose=0)\n","\n","        self.pred_model_update_counter += 1\n","        if (self.pred_model_update_counter >= UPDATE_PREDICTION_EVERY):\n","            self.update_prediction_model()\n","            self.pred_model_update_counter = 0\n","\n","    def update_prediction_model(self):\n","        self.prediction_model.set_weights(self.target_model.get_weights())\n","\n","    def update_epsilon(self):\n","        self.EPSILON = max(self.EPSILON* EPSILON_DECAY,MIN_EPSILON)\n","\n","    def run(self):\n","      self.train()\n","\n","    def load_model(self):\n","        self.target_model = load_model(MODEL_FILE_PATH)\n","        self.prediction_model = load_model(MODEL_FILE_PATH)\n","\n","    def train(self):\n","        start_tic = time.time()\n","        for epi in range(NUM_EPISODES):\n","\n","            state = env.reset()\n","            state = np.reshape(state, (1,STATE_SHAPE))\n","            done = False\n","\n","            reward_total = 0\n","            steps = 0\n","            tic = time.time()\n","            while not done:\n","\n","                # select an action with epsilon-greedy\n","                if np.random.binomial(1, self.EPSILON):\n","                    action = np.random.randint(NUM_ACTIONS)\n","                else:\n","                    action = np.argmax(self.prediction_model.predict(state)[0])\n","                self.update_epsilon()\n","\n","                # step wth the selected action\n","                new_state, reward, done, _= env.step(action)\n","                new_state = np.reshape(new_state, (1,STATE_SHAPE))\n","                steps += 1\n","                reward_total += reward\n","\n","                # append this instance to the memory and train the model\n","                self.replay_memory.append((state, action, reward, new_state, done))\n","                self.replay()\n","\n","                # set the state to new state\n","\n","                state = new_state\n","            # Computing time\n","\n","            tac = time.time()\n","            time_elapse, time_remain = compute_time(tic,tac,epi)\n","            print('episode {}: \\t reward: {} \\t time: {} \\t esimated time remaining: {}'.format(epi, reward_total, time_elapse, time_remain))\n","            if epi % SAVE_MODEL_EVERY == 0:\n","              self.save_model()\n","        end_tac = time.time()\n","        self.save_model()\n","        time_elapse_total, _useless = compute_time(start_tic, end_tac, NUM_EPISODES)\n","        print('total time used: {}'.format(time_elapse_total))\n","\n","    def render(self):\n","        state = env.reset()\n","        done = False\n","        while not done:\n","            action = np.argmax(self.target_model.predict(state.reshape(1,STATE_SHAPE)))\n","            new_state, reward, done, _ = env.step(action)\n","            env.render()\n","            state = new_state\n","\n","    def save_model(self):\n","        if os.path.exists(MODEL_FILE_PATH):\n","          shutil.rmtree(MODEL_FILE_PATH)\n","        self.target_model.save(MODEL_FILE_PATH)\n","\n","def compute_time(tic, tac, epi):\n","    total_time = tac - tic\n","    time_elapse = '{:.2f} seconds'.format(tac - tic)\n","    time_remaining = int((NUM_EPISODES - epi) * total_time)\n","    hours = time_remaining // 3600\n","    time_remaining %= 3600\n","    minutes = time_remaining // 60\n","    seconds = time_remaining % 60\n","    time_remain = '{:02.0f}:{:02.0f}:{:02.0f}'.format(hours, minutes, seconds)\n","    return time_elapse, time_remain\n","\n","\n","if __name__ == \"__main__\":\n","    agent = DQNAgent()\n","    agent.run()\n","    env.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["episode 0: \t reward: 16.0 \t time: 0.18 seconds \t esimated time remaining: 00:03:03\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/DQN_Model/assets\n","episode 1: \t reward: 16.0 \t time: 0.51 seconds \t esimated time remaining: 00:08:30\n","episode 2: \t reward: 15.0 \t time: 0.53 seconds \t esimated time remaining: 00:08:46\n","episode 3: \t reward: 12.0 \t time: 0.44 seconds \t esimated time remaining: 00:07:20\n","episode 4: \t reward: 11.0 \t time: 0.38 seconds \t esimated time remaining: 00:06:20\n","episode 5: \t reward: 8.0 \t time: 0.33 seconds \t esimated time remaining: 00:05:24\n","episode 6: \t reward: 10.0 \t time: 0.40 seconds \t esimated time remaining: 00:06:39\n","episode 7: \t reward: 9.0 \t time: 0.28 seconds \t esimated time remaining: 00:04:38\n","episode 8: \t reward: 8.0 \t time: 0.32 seconds \t esimated time remaining: 00:05:13\n","episode 9: \t reward: 9.0 \t time: 0.37 seconds \t esimated time remaining: 00:06:06\n","episode 10: \t reward: 11.0 \t time: 0.44 seconds \t esimated time remaining: 00:07:12\n","episode 11: \t reward: 9.0 \t time: 0.35 seconds \t esimated time remaining: 00:05:44\n","episode 12: \t reward: 8.0 \t time: 0.32 seconds \t esimated time remaining: 00:05:12\n","episode 13: \t reward: 9.0 \t time: 0.39 seconds \t esimated time remaining: 00:06:22\n","episode 14: \t reward: 10.0 \t time: 0.41 seconds \t esimated time remaining: 00:06:40\n","episode 15: \t reward: 16.0 \t time: 0.56 seconds \t esimated time remaining: 00:09:11\n","episode 16: \t reward: 8.0 \t time: 0.33 seconds \t esimated time remaining: 00:05:20\n","episode 17: \t reward: 10.0 \t time: 0.43 seconds \t esimated time remaining: 00:07:03\n","episode 18: \t reward: 8.0 \t time: 0.32 seconds \t esimated time remaining: 00:05:11\n","episode 19: \t reward: 10.0 \t time: 0.43 seconds \t esimated time remaining: 00:07:00\n","episode 20: \t reward: 10.0 \t time: 0.45 seconds \t esimated time remaining: 00:07:20\n","episode 21: \t reward: 9.0 \t time: 0.39 seconds \t esimated time remaining: 00:06:18\n","episode 22: \t reward: 9.0 \t time: 0.37 seconds \t esimated time remaining: 00:06:02\n","episode 23: \t reward: 8.0 \t time: 0.32 seconds \t esimated time remaining: 00:05:13\n","episode 24: \t reward: 9.0 \t time: 0.36 seconds \t esimated time remaining: 00:05:54\n","episode 25: \t reward: 9.0 \t time: 0.60 seconds \t esimated time remaining: 00:09:43\n","episode 26: \t reward: 10.0 \t time: 0.38 seconds \t esimated time remaining: 00:06:12\n","episode 27: \t reward: 9.0 \t time: 0.32 seconds \t esimated time remaining: 00:05:12\n","episode 28: \t reward: 21.0 \t time: 0.75 seconds \t esimated time remaining: 00:12:08\n","episode 29: \t reward: 10.0 \t time: 0.39 seconds \t esimated time remaining: 00:06:20\n","episode 30: \t reward: 8.0 \t time: 0.31 seconds \t esimated time remaining: 00:04:58\n","episode 31: \t reward: 15.0 \t time: 0.60 seconds \t esimated time remaining: 00:09:45\n","episode 32: \t reward: 9.0 \t time: 0.35 seconds \t esimated time remaining: 00:05:35\n","episode 33: \t reward: 10.0 \t time: 0.41 seconds \t esimated time remaining: 00:06:32\n","episode 34: \t reward: 8.0 \t time: 0.32 seconds \t esimated time remaining: 00:05:04\n","episode 35: \t reward: 9.0 \t time: 0.31 seconds \t esimated time remaining: 00:05:03\n","episode 36: \t reward: 10.0 \t time: 0.41 seconds \t esimated time remaining: 00:06:35\n","episode 37: \t reward: 8.0 \t time: 0.31 seconds \t esimated time remaining: 00:05:01\n","episode 38: \t reward: 9.0 \t time: 0.35 seconds \t esimated time remaining: 00:05:33\n","episode 39: \t reward: 10.0 \t time: 0.40 seconds \t esimated time remaining: 00:06:28\n","episode 40: \t reward: 9.0 \t time: 0.36 seconds \t esimated time remaining: 00:05:47\n","episode 41: \t reward: 8.0 \t time: 0.32 seconds \t esimated time remaining: 00:05:09\n","episode 42: \t reward: 9.0 \t time: 0.38 seconds \t esimated time remaining: 00:06:05\n","episode 43: \t reward: 9.0 \t time: 0.35 seconds \t esimated time remaining: 00:05:35\n","episode 44: \t reward: 9.0 \t time: 0.36 seconds \t esimated time remaining: 00:05:45\n","episode 45: \t reward: 11.0 \t time: 0.45 seconds \t esimated time remaining: 00:07:14\n","episode 46: \t reward: 8.0 \t time: 0.33 seconds \t esimated time remaining: 00:05:10\n","episode 47: \t reward: 9.0 \t time: 0.37 seconds \t esimated time remaining: 00:05:57\n","episode 48: \t reward: 10.0 \t time: 0.32 seconds \t esimated time remaining: 00:05:08\n","episode 49: \t reward: 9.0 \t time: 0.34 seconds \t esimated time remaining: 00:05:25\n","episode 50: \t reward: 10.0 \t time: 0.42 seconds \t esimated time remaining: 00:06:36\n","episode 51: \t reward: 12.0 \t time: 0.47 seconds \t esimated time remaining: 00:07:30\n","episode 52: \t reward: 9.0 \t time: 0.39 seconds \t esimated time remaining: 00:06:11\n","episode 53: \t reward: 8.0 \t time: 0.34 seconds \t esimated time remaining: 00:05:26\n","episode 54: \t reward: 10.0 \t time: 0.36 seconds \t esimated time remaining: 00:05:42\n","episode 55: \t reward: 9.0 \t time: 0.36 seconds \t esimated time remaining: 00:05:37\n","episode 56: \t reward: 10.0 \t time: 0.37 seconds \t esimated time remaining: 00:05:53\n","episode 57: \t reward: 8.0 \t time: 0.32 seconds \t esimated time remaining: 00:05:04\n","episode 58: \t reward: 9.0 \t time: 0.36 seconds \t esimated time remaining: 00:05:36\n","episode 59: \t reward: 9.0 \t time: 0.37 seconds \t esimated time remaining: 00:05:46\n","episode 60: \t reward: 9.0 \t time: 0.36 seconds \t esimated time remaining: 00:05:35\n","episode 61: \t reward: 9.0 \t time: 0.37 seconds \t esimated time remaining: 00:05:45\n","episode 62: \t reward: 9.0 \t time: 0.39 seconds \t esimated time remaining: 00:06:01\n","episode 63: \t reward: 14.0 \t time: 0.54 seconds \t esimated time remaining: 00:08:30\n","episode 64: \t reward: 9.0 \t time: 0.36 seconds \t esimated time remaining: 00:05:38\n","episode 65: \t reward: 9.0 \t time: 0.37 seconds \t esimated time remaining: 00:05:41\n","episode 66: \t reward: 9.0 \t time: 0.31 seconds \t esimated time remaining: 00:04:51\n","episode 67: \t reward: 10.0 \t time: 0.40 seconds \t esimated time remaining: 00:06:12\n","episode 68: \t reward: 9.0 \t time: 0.35 seconds \t esimated time remaining: 00:05:26\n","episode 69: \t reward: 9.0 \t time: 0.37 seconds \t esimated time remaining: 00:05:40\n","episode 70: \t reward: 9.0 \t time: 0.36 seconds \t esimated time remaining: 00:05:35\n","episode 71: \t reward: 8.0 \t time: 0.31 seconds \t esimated time remaining: 00:04:47\n","episode 72: \t reward: 9.0 \t time: 0.36 seconds \t esimated time remaining: 00:05:30\n","episode 73: \t reward: 8.0 \t time: 0.33 seconds \t esimated time remaining: 00:05:04\n","episode 74: \t reward: 9.0 \t time: 0.35 seconds \t esimated time remaining: 00:05:22\n","episode 75: \t reward: 9.0 \t time: 0.35 seconds \t esimated time remaining: 00:05:24\n","episode 76: \t reward: 9.0 \t time: 0.33 seconds \t esimated time remaining: 00:05:01\n","episode 77: \t reward: 8.0 \t time: 0.31 seconds \t esimated time remaining: 00:04:50\n","episode 78: \t reward: 10.0 \t time: 0.40 seconds \t esimated time remaining: 00:06:06\n","episode 79: \t reward: 10.0 \t time: 0.39 seconds \t esimated time remaining: 00:05:58\n","episode 80: \t reward: 9.0 \t time: 0.37 seconds \t esimated time remaining: 00:05:44\n","episode 81: \t reward: 10.0 \t time: 0.41 seconds \t esimated time remaining: 00:06:15\n","episode 82: \t reward: 11.0 \t time: 0.43 seconds \t esimated time remaining: 00:06:37\n","episode 83: \t reward: 10.0 \t time: 0.39 seconds \t esimated time remaining: 00:05:55\n","episode 84: \t reward: 9.0 \t time: 0.36 seconds \t esimated time remaining: 00:05:33\n","episode 85: \t reward: 9.0 \t time: 0.36 seconds \t esimated time remaining: 00:05:26\n","episode 86: \t reward: 10.0 \t time: 0.40 seconds \t esimated time remaining: 00:06:08\n","episode 87: \t reward: 10.0 \t time: 0.41 seconds \t esimated time remaining: 00:06:13\n","episode 88: \t reward: 15.0 \t time: 0.78 seconds \t esimated time remaining: 00:11:53\n","episode 89: \t reward: 8.0 \t time: 0.32 seconds \t esimated time remaining: 00:04:55\n","episode 90: \t reward: 9.0 \t time: 0.36 seconds \t esimated time remaining: 00:05:27\n","episode 91: \t reward: 10.0 \t time: 0.41 seconds \t esimated time remaining: 00:06:09\n","episode 92: \t reward: 9.0 \t time: 0.37 seconds \t esimated time remaining: 00:05:31\n","episode 93: \t reward: 10.0 \t time: 0.41 seconds \t esimated time remaining: 00:06:16\n","episode 94: \t reward: 9.0 \t time: 0.37 seconds \t esimated time remaining: 00:05:34\n","episode 95: \t reward: 8.0 \t time: 0.34 seconds \t esimated time remaining: 00:05:07\n","episode 96: \t reward: 10.0 \t time: 0.41 seconds \t esimated time remaining: 00:06:15\n","episode 97: \t reward: 10.0 \t time: 0.40 seconds \t esimated time remaining: 00:06:00\n","episode 98: \t reward: 9.0 \t time: 0.36 seconds \t esimated time remaining: 00:05:27\n","episode 99: \t reward: 9.0 \t time: 0.39 seconds \t esimated time remaining: 00:05:52\n","episode 100: \t reward: 11.0 \t time: 0.45 seconds \t esimated time remaining: 00:06:44\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/DQN_Model/assets\n","episode 101: \t reward: 8.0 \t time: 0.33 seconds \t esimated time remaining: 00:04:53\n","episode 102: \t reward: 9.0 \t time: 1.38 seconds \t esimated time remaining: 00:20:40\n","episode 103: \t reward: 10.0 \t time: 1.48 seconds \t esimated time remaining: 00:22:11\n","episode 104: \t reward: 8.0 \t time: 1.20 seconds \t esimated time remaining: 00:17:58\n","episode 105: \t reward: 9.0 \t time: 1.36 seconds \t esimated time remaining: 00:20:14\n","episode 106: \t reward: 10.0 \t time: 1.52 seconds \t esimated time remaining: 00:22:42\n","episode 107: \t reward: 10.0 \t time: 1.49 seconds \t esimated time remaining: 00:22:07\n","episode 108: \t reward: 9.0 \t time: 1.42 seconds \t esimated time remaining: 00:21:05\n","episode 109: \t reward: 10.0 \t time: 1.51 seconds \t esimated time remaining: 00:22:24\n","episode 110: \t reward: 9.0 \t time: 1.36 seconds \t esimated time remaining: 00:20:12\n","episode 111: \t reward: 8.0 \t time: 1.28 seconds \t esimated time remaining: 00:18:54\n","episode 112: \t reward: 10.0 \t time: 1.67 seconds \t esimated time remaining: 00:24:46\n","episode 113: \t reward: 9.0 \t time: 1.36 seconds \t esimated time remaining: 00:20:09\n","episode 114: \t reward: 8.0 \t time: 1.22 seconds \t esimated time remaining: 00:17:59\n","episode 115: \t reward: 10.0 \t time: 1.56 seconds \t esimated time remaining: 00:22:59\n","episode 116: \t reward: 8.0 \t time: 1.21 seconds \t esimated time remaining: 00:17:46\n","episode 117: \t reward: 9.0 \t time: 1.39 seconds \t esimated time remaining: 00:20:25\n","episode 118: \t reward: 11.0 \t time: 1.72 seconds \t esimated time remaining: 00:25:16\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"8wjnFT0AW2_t","outputId":"20b8b4ce-415d-4461-e9a5-c78b1dd4bc79"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]}]}